# KLA Python API ä½¿ç”¨æŒ‡å—

## ğŸ“– ç›®å½•

1. [åŸºç¡€ä½¿ç”¨](#åŸºç¡€ä½¿ç”¨)
2. [æ ‡å‡†KLAç®—æ³•](#æ ‡å‡†klaç®—æ³•)
3. [Warm-startå¢å¼º](#warm-startå¢å¼º)
4. [è‡ªå®šä¹‰ä¼˜åŒ–é—®é¢˜](#è‡ªå®šä¹‰ä¼˜åŒ–é—®é¢˜)
5. [APIå‚è€ƒ](#apiå‚è€ƒ)

---

## åŸºç¡€ä½¿ç”¨

### å®‰è£…å’Œå¯¼å…¥

```python
# ç¡®ä¿å·²å®‰è£…ä¾èµ–
# pip install -r requirements.txt

import sys
import os
# å¦‚æœä¸åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œéœ€è¦æ·»åŠ è·¯å¾„
sys.path.insert(0, '/path/to/KLA')

from src import kla_optimize, cost, Solution
```

---

## æ ‡å‡†KLAç®—æ³•

### 1. æœ€ç®€å•çš„ä½¿ç”¨

```python
from src import kla_optimize, cost

# è¿è¡ŒKLAç®—æ³•ä¼˜åŒ–æµ‹è¯•å‡½æ•°1
best_sol, history = kla_optimize(
    cost_function=cost,
    n_var=30,           # 30ç»´å˜é‡
    var_min=-100,       # ä¸‹ç•Œ
    var_max=100,        # ä¸Šç•Œ
    func_num=1          # æµ‹è¯•å‡½æ•°ç¼–å·
)

print(f"æœ€ä¼˜æˆæœ¬: {best_sol.cost}")
print(f"æœ€ä¼˜è§£ä½ç½®: {best_sol.position}")
```

### 2. è‡ªå®šä¹‰å‚æ•°

```python
best_sol, history = kla_optimize(
    cost_function=cost,
    n_var=30,
    var_min=-100,
    var_max=100,
    max_it=5000,        # æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤3000ï¼‰
    n_pop=100,          # ç§ç¾¤å¤§å°ï¼ˆé»˜è®¤50ï¼‰
    func_num=2
)

# æŸ¥çœ‹æ”¶æ•›å†å²
import matplotlib.pyplot as plt
plt.plot(history)
plt.xlabel('è¿­ä»£æ¬¡æ•°')
plt.ylabel('æœ€ä¼˜æˆæœ¬')
plt.yscale('log')
plt.show()
```

### 3. å¤šæ¬¡è¿è¡Œå¯¹æ¯”

```python
import numpy as np

results = []
for run in range(10):
    np.random.seed(run)
    best_sol, _ = kla_optimize(
        cost_function=cost,
        n_var=30,
        var_min=-100,
        var_max=100,
        func_num=1
    )
    results.append(best_sol.cost)

print(f"å¹³å‡æˆæœ¬: {np.mean(results)}")
print(f"æœ€ä¼˜æˆæœ¬: {np.min(results)}")
print(f"æ ‡å‡†å·®: {np.std(results)}")
```

---

## Warm-startå¢å¼º

### 1. è®­ç»ƒSurrogateæ¨¡å‹

```python
from src.warmstart import MetaSurrogate, generate_meta_training_data

# æ­¥éª¤1: ç”Ÿæˆå…ƒè®­ç»ƒæ•°æ®
print("ç”Ÿæˆè®­ç»ƒæ•°æ®...")
D_meta = generate_meta_training_data(
    n_tasks=50,              # ä»»åŠ¡æ•°é‡
    n_samples_per_task=2000, # æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°
    n_var=30,
    var_min=-100,
    var_max=100
)

# æ­¥éª¤2: åˆ›å»ºå¹¶è®­ç»ƒsurrogateæ¨¡å‹
print("è®­ç»ƒSurrogateæ¨¡å‹...")
surrogate = MetaSurrogate(
    model_type='mlp',                    # 'mlp' æˆ– 'rf' (éšæœºæ£®æ—)
    hidden_layers=(256, 128, 64, 32)     # MLPçš„éšè—å±‚ç»“æ„
)

surrogate.train(D_meta, normalize_y=True, verbose=True)

# æ­¥éª¤3: ä¿å­˜æ¨¡å‹ä¾›åç»­ä½¿ç”¨
surrogate.save('models/my_surrogate.pkl')
```

### 2. åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹

```python
from src.warmstart import MetaSurrogate

surrogate = MetaSurrogate()
surrogate.load('models/my_surrogate.pkl')
```

### 3. ä½¿ç”¨Warm-startè¿è¡ŒKLA

```python
from src import kla_optimize, cost

best_sol, history = kla_optimize(
    cost_function=cost,
    n_var=30,
    var_min=-100,
    var_max=100,
    func_num=1,
    # Warm-startå‚æ•°
    surrogate=surrogate,
    use_warm_start=True,
    warm_start_params={
        'n_cand': 2000,              # å€™é€‰ç‚¹æ•°é‡
        'alpha_mix': 0.5,            # éšæœºæ··åˆæ¯”ä¾‹(0.5=50%éšæœº+50%surrogate)
        'sampling_method': 'lhs',    # 'uniform', 'lhs', 'sobol'
        'diversity_threshold': None, # Noneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—
        'verbose': True              # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    }
)
```

### 4. å®Œæ•´çš„Warm-startå·¥ä½œæµ

```python
from src import kla_optimize, cost
from src.warmstart import MetaSurrogate, generate_meta_training_data
import numpy as np

# 1. è®­ç»ƒï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
try:
    surrogate = MetaSurrogate()
    surrogate.load('models/my_surrogate.pkl')
    print("åŠ è½½å·²æœ‰æ¨¡å‹")
except:
    print("è®­ç»ƒæ–°æ¨¡å‹...")
    D_meta = generate_meta_training_data(n_tasks=50, n_samples_per_task=2000)
    surrogate = MetaSurrogate(model_type='mlp', hidden_layers=(256, 128, 64, 32))
    surrogate.train(D_meta, normalize_y=True)
    surrogate.save('models/my_surrogate.pkl')

# 2. å¯¹æ¯”æ ‡å‡†KLAå’ŒWarm-start KLA
results_std = []
results_ws = []

for run in range(5):
    np.random.seed(run)
    
    # æ ‡å‡†KLA
    best_std, _ = kla_optimize(
        cost_function=cost, n_var=30, var_min=-100, var_max=100,
        func_num=1, use_warm_start=False
    )
    results_std.append(best_std.cost)
    
    # Warm-start KLA
    best_ws, _ = kla_optimize(
        cost_function=cost, n_var=30, var_min=-100, var_max=100,
        func_num=1, surrogate=surrogate, use_warm_start=True,
        warm_start_params={'n_cand': 2000, 'alpha_mix': 0.5}
    )
    results_ws.append(best_ws.cost)

print(f"æ ‡å‡†KLAå¹³å‡: {np.mean(results_std):.6e}")
print(f"Warm-startå¹³å‡: {np.mean(results_ws):.6e}")
print(f"æ”¹è¿›: {(np.mean(results_std) - np.mean(results_ws))/np.mean(results_std)*100:.2f}%")
```

---

## è‡ªå®šä¹‰ä¼˜åŒ–é—®é¢˜

### 1. å®šä¹‰è‡ªå·±çš„ç›®æ ‡å‡½æ•°

```python
import numpy as np

def my_objective_function(x, problem_params=None):
    """
    è‡ªå®šä¹‰ç›®æ ‡å‡½æ•°
    
    å‚æ•°:
        x: numpy.ndarray, å½¢çŠ¶ (n_samples, n_dimensions)
        problem_params: å¯é€‰çš„é—®é¢˜å‚æ•°
    
    è¿”å›:
        y: numpy.ndarray, å½¢çŠ¶ (n_samples,)
    """
    # ä¾‹å¦‚ï¼šRosenbrockå‡½æ•°
    result = np.zeros(x.shape[0])
    for i in range(x.shape[1] - 1):
        result += 100 * (x[:, i+1] - x[:, i]**2)**2 + (1 - x[:, i])**2
    return result

# ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
from src import kla_optimize

best_sol, history = kla_optimize(
    cost_function=my_objective_function,
    n_var=10,
    var_min=-5,
    var_max=10,
    max_it=3000,
    n_pop=50,
    func_num=None  # è‡ªå®šä¹‰å‡½æ•°æ—¶å¯ä»¥ä¼ None
)
```

### 2. å¸¦å‚æ•°çš„ç›®æ ‡å‡½æ•°

```python
def parametric_function(x, params):
    """å¸¦å‚æ•°çš„ç›®æ ‡å‡½æ•°"""
    a, b, c = params['a'], params['b'], params['c']
    return a * np.sum(x**2, axis=1) + b * np.sum(x, axis=1) + c

# åˆ›å»ºåŒ…è£…å‡½æ•°
params = {'a': 2, 'b': -1, 'c': 10}
cost_func = lambda x, fn: parametric_function(x, params)

best_sol, _ = kla_optimize(
    cost_function=cost_func,
    n_var=20,
    var_min=-10,
    var_max=10
)
```

### 3. çº¦æŸä¼˜åŒ–ï¼ˆæƒ©ç½šå‡½æ•°æ³•ï¼‰

```python
def constrained_objective(x, penalty_weight=1000):
    """
    å¸¦çº¦æŸçš„ç›®æ ‡å‡½æ•°
    ä½¿ç”¨æƒ©ç½šå‡½æ•°æ³•å¤„ç†çº¦æŸ
    """
    # åŸå§‹ç›®æ ‡å‡½æ•°
    f = np.sum(x**2, axis=1)
    
    # çº¦æŸ1: x1 + x2 <= 5
    g1 = x[:, 0] + x[:, 1] - 5
    penalty1 = penalty_weight * np.maximum(0, g1)**2
    
    # çº¦æŸ2: x1 >= 0
    g2 = -x[:, 0]
    penalty2 = penalty_weight * np.maximum(0, g2)**2
    
    return f + penalty1 + penalty2

best_sol, _ = kla_optimize(
    cost_function=lambda x, fn: constrained_objective(x),
    n_var=5,
    var_min=-10,
    var_max=10
)
```

---

## APIå‚è€ƒ

### `kla_optimize()`

**ä¸»è¦å‡½æ•°ï¼šè¿è¡ŒKLAä¼˜åŒ–ç®—æ³•**

```python
def kla_optimize(
    cost_function,      # ç›®æ ‡å‡½æ•°
    n_var,              # å†³ç­–å˜é‡æ•°é‡
    var_min,            # å˜é‡ä¸‹ç•Œ
    var_max,            # å˜é‡ä¸Šç•Œ
    max_it=3000,        # æœ€å¤§å‡½æ•°è¯„ä¼°æ¬¡æ•°
    n_pop=50,           # ç§ç¾¤å¤§å°
    func_num=1,         # æµ‹è¯•å‡½æ•°ç¼–å·
    surrogate=None,     # Surrogateæ¨¡å‹ï¼ˆç”¨äºwarm-startï¼‰
    use_warm_start=False,           # æ˜¯å¦ä½¿ç”¨warm-start
    warm_start_params=None          # Warm-startå‚æ•°å­—å…¸
)
```

**è¿”å›å€¼:**
- `best_sol`: Solutionå¯¹è±¡ï¼ŒåŒ…å«`.position`å’Œ`.cost`å±æ€§
- `best_cost_history`: numpyæ•°ç»„ï¼Œè®°å½•æ¯æ¬¡è¿­ä»£çš„æœ€ä¼˜æˆæœ¬

**ç¤ºä¾‹:**
```python
best_sol, history = kla_optimize(
    cost_function=my_func,
    n_var=30,
    var_min=-100,
    var_max=100
)
```

### `MetaSurrogate`

**å…ƒå­¦ä¹ ä»£ç†æ¨¡å‹ç±»**

```python
from src.warmstart import MetaSurrogate

# åˆ›å»ºæ¨¡å‹
surrogate = MetaSurrogate(
    model_type='mlp',              # 'mlp' æˆ– 'rf'
    hidden_layers=(256, 128, 64),  # MLPéšè—å±‚
    random_state=42                # éšæœºç§å­
)

# è®­ç»ƒæ¨¡å‹
surrogate.train(D_meta, normalize_y=True, verbose=True)

# é¢„æµ‹
predictions = surrogate.predict(X_candidates)

# ä¿å­˜/åŠ è½½
surrogate.save('path/to/model.pkl')
surrogate.load('path/to/model.pkl')
```

### `generate_meta_training_data()`

**ç”Ÿæˆå…ƒè®­ç»ƒæ•°æ®**

```python
from src.warmstart import generate_meta_training_data

D_meta = generate_meta_training_data(
    n_tasks=50,              # ä»»åŠ¡æ•°é‡
    n_samples_per_task=2000, # æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°
    n_var=30,                # å˜é‡ç»´åº¦
    var_min=-100,            # ä¸‹ç•Œ
    var_max=100              # ä¸Šç•Œ
)
```

**è¿”å›å€¼:**
- `D_meta`: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å­—å…¸ `{'X': numpy.ndarray, 'y': numpy.ndarray}`

### `warm_start_initialization()`

**ç”Ÿæˆwarm-startåˆå§‹ç§ç¾¤**

```python
from src.warmstart import warm_start_initialization

X_init = warm_start_initialization(
    surrogate=surrogate,
    search_space=(var_min, var_max, n_var),
    n_pop=50,
    n_cand=2000,
    alpha_mix=0.5,
    diversity_threshold=None,
    sampling_method='lhs',
    verbose=True
)
```

### `cost()`

**å†…ç½®æµ‹è¯•å‡½æ•°**

```python
from src import cost

# å‡½æ•°1: å¹³ç§»çƒé¢å‡½æ•°
y1 = cost(X, jj=1)

# å‡½æ•°2: Schwefelé—®é¢˜1.2
y2 = cost(X, jj=2)

# å‡½æ•°3: å¸¦å™ªå£°çš„Schwefelé—®é¢˜1.2
y3 = cost(X, jj=3)
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„å‚æ•°ï¼Ÿ

**åŸºæœ¬å‚æ•°å»ºè®®:**
- `n_pop`: é€šå¸¸30-100ä¹‹é—´ï¼Œé—®é¢˜è¶Šå¤æ‚å¯ä»¥é€‚å½“å¢å¤§
- `max_it`: æ ¹æ®è®¡ç®—é¢„ç®—ï¼Œå»ºè®®è‡³å°‘1000æ¬¡
- `var_min/var_max`: æ ¹æ®å®é™…é—®é¢˜çš„æœç´¢èŒƒå›´è®¾å®š

**Warm-startå‚æ•°å»ºè®®:**
- `alpha_mix`: 0.3-0.5è¾ƒå¥½ï¼ˆ30-50%éšæœºï¼‰
- `n_cand`: 10-50å€ç§ç¾¤å¤§å°
- `sampling_method`: 'lhs'é€šå¸¸ä¼˜äº'uniform'

### Q2: Warm-startä»€ä¹ˆæ—¶å€™æœ‰æ•ˆï¼Ÿ

**é€‚ç”¨åœºæ™¯:**
âœ… ä½ç»´é—®é¢˜ï¼ˆ<10ç»´ï¼‰
âœ… æœ‰ç›¸ä¼¼å†å²ä¼˜åŒ–ç»éªŒ
âœ… è®¡ç®—æˆæœ¬é«˜çš„é»‘ç›’å‡½æ•°
âœ… éœ€è¦å¿«é€Ÿæ”¶æ•›çš„åœºæ™¯

**ä¸é€‚ç”¨åœºæ™¯:**
âŒ é«˜ç»´ç®€å•æµ‹è¯•å‡½æ•°
âŒ ç®—æ³•æœ¬èº«å·²ç»å¾ˆå¼º
âŒ æœ‰å……è¶³çš„è¿­ä»£é¢„ç®—

### Q3: å¦‚ä½•åŠ é€Ÿè®­ç»ƒï¼Ÿ

```python
# 1. å‡å°‘ä»»åŠ¡æ•°å’Œæ ·æœ¬æ•°
D_meta = generate_meta_training_data(n_tasks=20, n_samples_per_task=1000)

# 2. ä½¿ç”¨éšæœºæ£®æ—ä»£æ›¿MLPï¼ˆæ›´å¿«ä½†å¯èƒ½ç²¾åº¦ä½ï¼‰
surrogate = MetaSurrogate(model_type='rf')

# 3. å‡å°ç½‘ç»œè§„æ¨¡
surrogate = MetaSurrogate(hidden_layers=(128, 64))
```

### Q4: å¦‚ä½•å¤„ç†ä¸åŒå°ºåº¦çš„å˜é‡ï¼Ÿ

```python
# æ–¹æ³•1: æ ‡å‡†åŒ–åˆ°[-1, 1]
def standardize_vars(x_original, bounds_original):
    # bounds_original: [(min1, max1), (min2, max2), ...]
    x_std = np.zeros_like(x_original)
    for i, (vmin, vmax) in enumerate(bounds_original):
        x_std[:, i] = 2 * (x_original[:, i] - vmin) / (vmax - vmin) - 1
    return x_std

# æ–¹æ³•2: åœ¨ç›®æ ‡å‡½æ•°å†…éƒ¨å¤„ç†
def my_func_with_scaling(x, func_num):
    # x æ˜¯æ ‡å‡†åŒ–çš„å˜é‡
    x_original = x.copy()
    x_original[:, 0] = x[:, 0] * 1000  # ç¬¬ä¸€ä¸ªå˜é‡æ”¾å¤§1000å€
    x_original[:, 1] = x[:, 1] * 0.01  # ç¬¬äºŒä¸ªå˜é‡ç¼©å°100å€
    # ... è®¡ç®—ç›®æ ‡å‡½æ•°
    return result
```

---

## å®Œæ•´ç¤ºä¾‹

æŸ¥çœ‹ `examples/` ç›®å½•ä¸‹çš„å®Œæ•´ç¤ºä¾‹ï¼š

- `examples/kla_warmstart_demo.py` - å®Œæ•´çš„å¯¹æ¯”å®éªŒ
- `examples/test_improved_warmstart.py` - å¿«é€Ÿæµ‹è¯•è„šæœ¬

è¿è¡Œç¤ºä¾‹ï¼š
```bash
cd /path/to/KLA
python examples/test_improved_warmstart.py
```

---

## æŠ€æœ¯æ”¯æŒ

- ğŸ“– è¯¦ç»†æ–‡æ¡£: `docs/README_warmstart.md`
- ğŸ› é—®é¢˜æŠ¥å‘Š: GitHub Issues
- ğŸ“§ è”ç³»æ–¹å¼: Nimakhan@berkeley.edu
